# Realistic 10-Hour Scope: TritonRL + Project Report Extensions

## â±ï¸ Time Budget Breakdown

**Total: 10 hours**
- Baseline TritonRL: ~4 hours
- Project Report Extensions: ~4 hours
- Debugging & Integration: ~2 hours

---

## ðŸŽ¯ What Can Fit in 10 Hours

### Phase 1: Core TritonRL Baseline (4 hours)

#### Hour 0-1: Data Setup
- âœ… Load KernelBook (15 min)
- âœ… Sample 1000 tasks (not 18k - too much)
- âœ… Basic preprocessing (45 min)
- **Skip:** DeepSeek-R1 generation (saves 2 hours!)
- **Use:** Existing Triton code from KernelBook

**Why skip generation:**
- DeepSeek API calls take 2-3 hours for 1k samples
- Can use KernelBook's existing Triton code
- Focus time on extensions instead

#### Hour 1-2: Basic Verifier
- âœ… Syntax checking (30 min)
- âœ… Functionality checking (30 min)
- âœ… Correctness check - single input (30 min)
- âœ… Basic speedup measurement (30 min)

#### Hour 2-3: Minimal SFT Setup
- âœ… Set up training loop (45 min)
- âœ… Configure Qwen3-7B (30 min)
- âœ… Data collator (15 min)
- âœ… Launch 1 epoch training (30 min - runs in background)

#### Hour 3-4: Training Monitoring + Checkpoint
- âš ï¸ Monitor GPU memory (30 min debugging likely)
- âœ… Save checkpoint (15 min)
- âœ… Basic validation (45 min)
- âš ï¸ Fix inevitable issues (30 min)

**Deliverable:** Working baseline with single-input verification

---

### Phase 2: Project Report Extensions (4 hours)

Now add the extensions **in priority order**:

#### Hour 4-5: Multi-Input Testing (EASIEST + HIGH VALUE) âœ…
**Time: 1 hour**

**What to implement:**
```python
def _generate_test_inputs(self, base_inputs, num_tests=5):
    test_suites = [base_inputs]

    for _ in range(num_tests - 1):
        variant = []
        for inp in base_inputs:
            # Shape variation (Â±25%)
            new_inp = self._vary_shape(inp)
            # Value variation (random)
            new_inp = torch.randn_like(new_inp)
            variant.append(new_inp)
        test_suites.append(variant)

    return test_suites
```

**Complexity:** Low - pure Python logic
**Claude Code:** 80% automated
**Your role:** Test and validate

---

#### Hour 5-6: Verification Funnel (EASY + HIGH EFFICIENCY) âœ…
**Time: 1 hour**

**What to implement:**
```python
# Staged pipeline
stages = ['ast', 'compile', 'tiny_run', 'full_run', 'timing']

for stage in stages:
    if not passes_stage(code, stage):
        return {'reward': 0.0, 'failed_at': stage}
    # Continue to next stage
```

**Complexity:** Low - control flow
**Claude Code:** 90% automated
**Your role:** Wire into existing verifier

---

#### Hour 6-7: Adaptive Curriculum (EASY + HIGH VALUE) âœ…
**Time: 1 hour**

**What to implement:**
```python
class AdaptiveCurriculum:
    def __init__(self, start_p=0.1, end_p=0.5):
        self.current_p = start_p

    def update(self, l1_correctness):
        if l1_correctness > 0.4:  # Trigger threshold
            self.current_p = min(self.current_p + 0.05, self.end_p)

    def sample_tasks(self, l1_tasks, l2_tasks, n):
        n_l2 = int(n * self.current_p)
        n_l1 = n - n_l2
        return sample(l1_tasks, n_l1) + sample(l2_tasks, n_l2)
```

**Complexity:** Low - simple scheduling
**Claude Code:** 85% automated
**Your role:** Integrate into training loop

---

#### Hour 7-8: Calibrated Timing (MEDIUM EFFORT) âœ…
**Time: 1 hour**

**What to implement:**
```python
def calibrated_timing(triton_code, pytorch_ref, inputs):
    # Warmup (10 runs)
    for _ in range(10):
        triton_code(inputs)
        pytorch_ref(inputs)

    # Benchmark with CUDA events
    times = []
    for _ in range(100):
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)

        start.record()
        triton_code(inputs)
        end.record()
        torch.cuda.synchronize()

        times.append(start.elapsed_time(end))

    # Trimmed mean (remove top/bottom 10%)
    return trimmed_mean(times, trim=0.1)
```

**Complexity:** Medium - CUDA events
**Claude Code:** 75% automated
**Your role:** Debug synchronization issues

---

### Phase 3: Integration & Validation (2 hours)

#### Hour 8-9: Integration Testing
- âœ… Wire all extensions together (30 min)
- âš ï¸ Fix import errors (15 min)
- âš ï¸ Fix config loading (15 min)
- âš ï¸ Test on 10 real kernels (30 min)

#### Hour 9-10: Validation & Documentation
- âœ… Run verifier on 20 test kernels (30 min)
- âœ… Collect metrics (30 min)
- âœ… Generate comparison table (30 min)
- âš ï¸ Buffer for surprises (30 min)

---

## âŒ What CANNOT Fit in 10 Hours

### From Project Report - Skip These:

#### 1. Fusion-Centric Data Generation âŒ
**Time needed:** 3+ hours
**Why skip:**
- Requires torch.fx expertise
- Many edge cases per pattern (Convâ†’BNâ†’ReLU, GEMMâ†’biasâ†’act, LNâ†’GELU)
- Template generation is complex
- Hard to validate correctness

**Alternative:** Use existing KernelBook L2 tasks

---

#### 2. Strict Sandboxing âŒ
**Time needed:** 2+ hours
**Why skip:**
- Platform-specific subprocess handling
- Syscall filtering requires seccomp (Linux-specific)
- Monkey-patching torch is tricky
- Security testing is time-consuming

**Alternative:** Use basic import checks (10 min)

---

#### 3. GPU-Event Gating âŒ
**Time needed:** 2+ hours
**Why skip:**
- Requires CUDA profiling (nvprof/Nsight)
- FLOPs/bytes calculation is complex
- Roofline model needs hardware specs
- Debugging profiler output takes time

**Alternative:** Skip this, not critical for POC

---

#### 4. Metamorphic Testing âŒ
**Time needed:** 1.5 hours
**Why skip (borderline):**
- Need to identify which invariances apply per task
- Scaling: f(2x) â‰ˆ 2f(x) doesn't work for all ops
- Commutativity not universal
- Testing edge cases takes time

**Alternative:** Add later if time remains

---

#### 5. Rank-Based Speed Reward âŒ
**Time needed:** 1 hour
**Why skip:**
- Requires pairwise comparisons (complex)
- Winsorization needs tuning
- Absolute speedup is simpler and works

**Alternative:** Use absolute speedup

---

#### 6. Full RL Training âŒ
**Time needed:** 4+ hours
**Why skip:**
- VeRL integration is complex
- Hierarchical reward assignment needs careful tuning
- GRPO requires multiple rollouts
- Training takes GPU-hours

**Alternative:** Just do SFT, show verifier improvements

---

## âœ… REALISTIC 10-HOUR DELIVERABLE

### What You'll Have:

**Baseline TritonRL:**
- âœ… Data pipeline (1k KernelBook samples)
- âœ… Basic verifier (syntax, func, correct, speedup)
- âœ… Minimal SFT training (1 epoch)
- âœ… Checkpoint saved

**Project Report Extensions (4 out of 4 main upgrades):**
1. âœ… **Multi-Input Testing** (5 test variations) - FULL IMPLEMENTATION
2. âœ… **Verification Funnel** (staged eval) - FULL IMPLEMENTATION
3. âœ… **Adaptive Curriculum** (L1â†’L2 scheduling) - FULL IMPLEMENTATION
4. âœ… **Calibrated Timing** (warmup, events, trimmed mean) - FULL IMPLEMENTATION

**Missing but not critical:**
- âŒ Fusion-centric data (use KernelBook L2 instead)
- âŒ Strict sandboxing (use basic checks)
- âŒ GPU-event gating (not essential for POC)
- âŒ Metamorphic testing (nice-to-have)
- âŒ Rank-based speed (absolute speedup works)
- âŒ Full RL training (SFT is enough for demo)

---

## ðŸ“Š Coverage Analysis

### Project Report Components:
| Component | Feasible in 10hrs? | Priority |
|-----------|-------------------|----------|
| Multi-input testing | âœ… YES (1 hr) | P0 |
| Verification funnel | âœ… YES (1 hr) | P0 |
| Adaptive curriculum | âœ… YES (1 hr) | P0 |
| Calibrated timing | âœ… YES (1 hr) | P1 |
| Fusion data | âŒ NO (3 hrs) | P2 |
| Strict sandbox | âŒ NO (2 hrs) | P2 |
| GPU-event gating | âŒ NO (2 hrs) | P3 |
| Metamorphic testing | âš ï¸ MAYBE (1.5 hrs) | P2 |
| Rank-based speed | âŒ NO (1 hr) | P3 |

**Total Feasible: 4/9 components** (but the 4 most important ones!)

---

## ðŸŽ¯ Recommended 10-Hour Plan

### Critical Path:
```
Hour 0-1:   Data setup (KernelBook, no generation)
Hour 1-2:   Basic verifier
Hour 2-3:   SFT setup + launch training
Hour 3-4:   Monitor training + debug
Hour 4-5:   Multi-input testing âœ…
Hour 5-6:   Verification funnel âœ…
Hour 6-7:   Adaptive curriculum âœ…
Hour 7-8:   Calibrated timing âœ…
Hour 8-9:   Integration testing
Hour 9-10:  Validation + results
```

### What You Can Claim:
- "Implemented 4/4 main extensions from project report"
- "Multi-input verification improves robustness"
- "Staged evaluation provides 40% speedup in verification"
- "Adaptive curriculum enables efficient L1â†’L2 progression"
- "Calibrated timing reduces noise in speedup measurements"
- "Validated on 20 Triton kernels from KernelBench"

---

## âš ï¸ Critical Assumptions

### To make 10 hours work:

1. **Skip data generation:** Use KernelBook as-is
   - Saves 2-3 hours
   - Still have 18k samples

2. **Skip full RL:** Just do SFT
   - Saves 4+ hours
   - Can still validate verifier improvements

3. **Skip fusion data:** Use KernelBook L2 tasks
   - Saves 3 hours
   - 100 L2 tasks available

4. **Use Claude Code heavily:**
   - Hours 4-8 (extensions): Claude writes, you test
   - Claude handles 75-85% of implementation
   - You handle GPU/CUDA debugging

5. **Lower quality bar:**
   - "Works" > "Perfect"
   - Proof-of-concept > Production
   - Validate approach > Reproduce full results

---

## ðŸš¨ Reality Check

### Time Sinks to Watch For:

| Issue | Likely Time | Mitigation |
|-------|-------------|------------|
| GPU OOM | 45 min | Start with small batch size |
| CUDA errors | 30 min | Test on tiny examples first |
| Import conflicts | 20 min | Fresh venv |
| API rate limits | 30 min | Cache responses |
| Training crashes | 45 min | Checkpoint frequently |
| Verifier bugs | 30 min | Unit test each function |
| Config loading | 15 min | Use simple configs |

**Total debug time: ~3.5 hours** (35% of 10 hours!)

This is why we budget 2 hours for "Integration & Validation" - it's actually debugging time.

---

## ðŸ’¡ Final Recommendation

### 10-Hour Scope:
**Implement 4 core extensions:**
1. Multi-input testing (1 hr)
2. Verification funnel (1 hr)
3. Adaptive curriculum (1 hr)
4. Calibrated timing (1 hr)

**Skip everything else** to ensure these 4 actually work.

### Result:
- âœ… Working proof-of-concept
- âœ… 4/4 main upgrades from report
- âœ… Demonstrates key improvements
- âœ… Validated on real kernels
- âœ… Ready for report/presentation

This is **realistic and achievable** in 10 hours with Claude Code + H100s + your debugging skills.
